# ğŸš€ 14-Day PySpark Crash Course

A **14-day fast-track learning plan** to master **PySpark** for Data Engineering and Interview Preparation. This 
roadmap is designed for **Data Engineers & Data Scientists** preparing for projects or interviews.  
Each day includes **theory**, ** âœ… Tasks: coding**, and **interview Q&A**.


---

## ğŸ“… Learning Plan

### **Week 1 â€“ Introduction & Setup and Fundamentals**
### **Day 1: - [Intro to Spark & Setup](docs/day1.md)**
  - What is PySpark? Why use it?
  - Install PySpark (local & in Jupyter/Colab).
  - Spark architecture: Driver, Executors, Cluster Manager.
  - SparkSession, Cluster overview, PySpark installation
  - âœ… [Tasks: Run Spark locally, Explore SparkSession, Hello World](models/Day1.ipynb).

### **Day 2 â€“ [RDD Basics](docs/day2.md)**
- What are RDDs? Transformations vs Actions.
- Creating RDDs.
- `map`, `filter`, `reduce`, `collect`.
- [ âœ… Tasks: Word count example](models/Day2.ipynb).

### **Day 3 â€“ DataFrames Basics**
- Intro to Spark SQL & DataFrames.
- Creating DataFrames.
- Schema, dtypes, show(), select, filter, withColumn, schema.
- âœ… Tasks: Load CSV, Explore schema, Apply filters

### **Day 4 â€“ DataFrame Operations**
- Select, filter, withColumn.
- Sorting, distinct, dropDuplicates.
- âœ… Tasks:: Employee dataset cleaning.

### **Day 5 â€“ Aggregations & Joins & Unions**
- GroupBy, agg, count, sum, avg.
- Joins (inner, left, right, outer, full), Unions.
- âœ… Tasks: Compute average sales, Count unique customers
- âœ… Tasks: Join customers & orders, Union multiple datasets  

### **Day 6 â€“ SQL with PySpark**
- Register DataFrames as SQL tables.
- Run SQL queries with `spark.sql`, createOrReplaceTempView, SQL queries, functions 
- âœ… Tasks: Run SQL on DataFrame, Use SQL aggregations

### **Day 7 â€“ Data Cleaning & Transformation**
- Handling nulls (`dropna`, `fillna`).
- Casting, replacing values.
- Date & string functions.
-  âœ… Tasks:: Transform messy dataset.

### **Day 8 â€“ Window Functions**
- `row_number`, `rank`, `dense_rank`,  `lag`, `lead`.
- Partition & ordering
- âœ… Tasks: Top-N problem.
- âœ… Tasks: Find top 3 sales per region, Calculate moving average

### **Day 9 â€“ UDFs & Performance and Complex data**
- User Defined Functions (UDFs), map, structs, arrays  
- Pandas UDFs.
- When to avoid UDFs.
- âœ… Tasks: Custom transformations.
- âœ… Tasks: Write a UDF, Parse nested JSON fields

### **Day 10 â€“ Data Sources & File Formats**
- Reading/Writing CSV, JSON, Parquet, ORC, Delta.
- Partitioning & bucketing.
-  âœ… Tasks:: Save datasets in Parquet, Compare file sizes.

### **Day 11 â€“ Spark MLlib Basics**
- Intro to Spark MLlib.
- Feature Engineering.
- Train simple ML model.
-  âœ… Tasks:: Logistic Regression on dataset.

### **Day 12 â€“ Streaming with PySpark**
- Structured Streaming basics.
- Reading from Kafka/Socket.
- Writing to console & file.
-  âœ… Tasks:: Streaming word count.

### **Day 13 â€“ Optimization & Tuning and Execution Model & Spark UI**
- Lazy evaluation, Catalyst optimizer, Stages, Tasks, DAG.
- Repartition vs Coalesce.
- Caching & persistence.
- Best practices.
- âœ… Tasks: Run job & check Spark UI, Explain DAG execution

### **Day 14 â€“ Capstone Project + Interview Prep**
- #### Project 1 â€“ Sales ETL Pipeline
  - ETL, cleaning, joins, partitioned writes  
  - âœ… Tasks: Load sales data, Transform & clean, Write to Parquet
- #### Project 2 â€“ User Analytics
  - log parsing, window functions, aggregations  
  - âœ… Tasks: Parse JSON logs, Sessionize user activity, Find top users
- End-to-End ETL with PySpark:
  - Read raw data â†’ Clean â†’ Aggregate â†’ Store in Parquet.
- Mock interview Q&A:
  - Difference between RDD vs DataFrame.
  - How Spark executes a job.
  - Common optimization techniques.

---

## ğŸ› ï¸ Requirements
- Python 3.8+
- Java 8/11
- PySpark (`pip install pyspark`)
- Jupyter Notebook / VSCode /Pycharm

---

## ğŸ“š Resources
- [Spark Documentation](https://spark.apache.org/docs/latest/)
- [Databricks Learning](https://www.databricks.com/learn)

---

## ğŸ¯ Outcomes
By the end of this crash course, you will:
- Build ETL pipelines with PySpark.
- Perform aggregations, joins, and transformations.
- Work with streaming & batch data.
- Be ready for **Data Engineer / PySpark Interview**.

---
